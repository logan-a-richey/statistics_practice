\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{multicol}
\usepackage{enumitem}
\usepackage{titlesec}
\usepackage{helvet}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{hyperref}
\titleformat{\section}{\large\bfseries}{}{0em}{}[\titlerule]
\titlespacing{\section}{0pt}{0.5em}{0.25em}
\setlength{\parskip}{0.2em}
\setlength{\parindent}{0em}
\setlist[itemize]{noitemsep, topsep=0pt, left=1em}
\usepackage[none]{hyphenat}
\pagenumbering{gobble}
\usepackage{amsmath}

\vspace*{-2cm}
\begin{document}

\section{Binomial: Mean and Variance from MGF}

\subsection{MGF of Binomial Distribution}
The moment generating function (MGF) of a binomial distribution is given by:
\[
    M_X(t) = E[e^{tX}] = \sum_{x=0}^{n} e^{tx} P(X = x)
\]

We know that
\[
    P(X = x) = \binom{n}{x} p^x q^{n-x}, \quad \text{where } q = 1 - p
\]
Substituting in:
\[
    M_X(t) = \sum_{x=0}^{n} \binom{n}{x} (pe^t)^x q^{n-x}
\]
By the Binomial Theorem, this simplifies to:
\[
    M_X(t) = (q + pe^t)^n
\]

\subsection{Raw Moments}
We can find the first and second raw moments by differentiating the MGF.

\textbf{First moment (mean):}
\[
    \mu_1' = E[X] = \frac{dM_X(t)}{dt}\bigg|_{t=0}
\]
Differentiate:
\[
    \frac{dM_X(t)}{dt} = n(q + pe^t)^{n-1}(pe^t)
\]
Now evaluate at $t = 0$:
\[
    \mu_1' = np(q + p)^{n-1} = np(1)^{n-1} = np
\]
Therefore, the mean is:
\[
    \boxed{E[X] = np}
\]

\textbf{Second moment:}
\[
    \mu_2' = \frac{d^2 M_X(t)}{dt^2}\bigg|_{t=0}
\]
Differentiate again:
\[
    \frac{d^2 M_X(t)}{dt^2} = np \left[ e^t (n-1)(q + pe^t)^{n-2}(pe^t) + e^t (q + pe^t)^{n-1} \right]
\]
Simplify and evaluate at $t = 0$:
\[
    \mu_2' = np \left[ (n-1)p + 1 \right] = n(n-1)p^2 + np
\]

\subsection{Variance of Binomial Distribution}
The variance is given by:
\[
    \sigma^2 = E[X^2] - (E[X])^2 = \mu_2' - (\mu_1')^2
\]
Substitute the results:
\[
    \sigma^2 = \left[ n(n-1)p^2 + np \right] - (np)^2
\]
Simplify:
\[
    \sigma^2 = n(n-1)p^2 + np - n^2p^2 = np(1 - p)
\]
Thus,
\[
    \boxed{\sigma^2 = \operatorname{Var}(X) = npq}
\]
\pagebreak 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bernoulli: Mean and Variance from MGF}
\subsection{MGF of Bernoulli Distribution}
Given the moment-generating function (MGF):
\[
    M_X(t) = 1 - p + pe^t 
\]

\subsection{Raw Moments}
Let's find the first moment:
\[
    \dfrac{d}{dt} M_X(t) = \dfrac{d}{dt} (1 - p + pe^t) = pe^t
\]
\[
 	M_X'(0) = pe^0 
\]
Therefore, the mean is:
\[
    \boxed{E[X] = p}
\]

Let's find the second moment:
\[
    \dfrac{d^2}{dt^2} M_X(t) = \dfrac{d}{dt} (pe^t) = pe^t
\]

\subsection{Variance of Bernoulli Distribution}
Let's find the variance: $\sigma^2 = E[X^2] - E[X]^2$. Substituting yields: \\
\[
    \sigma^2 = p - p^2 = p(1 - p) = pq
\]
Therefore, the variance is: \\
\[
	\boxed{\sigma^2 = pq}
\]

\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Poisson: Mean and Variance from MGF}
\subsection{MGF of Poisson Distribution}

The Poisson distribution is discrete, with PMF:
\[ 
    P(X = x) = \frac{e^{-\lambda}\lambda^x}{x!}, \quad x = 0, 1, 2, \ldots 
\]

The moment generating function (MGF) is:
\[ 
    M_X(t) = E[e^{tX}] = \sum_{x=0}^{\infty} e^{tx} P(X = x) 
\]

Substitute the PMF:
\[ 
    M_X(t) = e^{-\lambda} \sum_{x=0}^{\infty} \frac{(\lambda e^t)^x}{x!} 
\]

Recognize this as the Taylor expansion of $e^{z}$:
\[ 
    \sum_{x=0}^{\infty} \frac{z^x}{x!} = e^z 
\]

Thus,
\[
    M_X(t) = e^{-\lambda} \cdot e^{\lambda e^t} = e^{\lambda(e^t - 1)} 
\]

\subsection{Raw Moments}
The mean and variance can be derived from derivatives of $M_X(t)$.

\textbf{First moment (mean):}
\[ 
    E[X] = M_X'(0) 
\]

Differentiate:
\[ 
    M_X'(t) = \lambda e^t e^{\lambda(e^t - 1)} = \lambda e^t M_X(t) 
\]

Evaluate at $t = 0$:
\[  
    E[X] = M_X'(0) = \lambda e^0 M_X(0) = \lambda 
\]

\textbf{Second moment:}
\[ 
    M_X''(t) = \frac{d}{dt}[\lambda e^t M_X(t)] = \lambda e^t M_X(t) + \lambda e^t M_X'(t) 
\]

Substitute $M_X'(t) = \lambda e^t M_X(t)$:
\[ 
    M_X''(t) = \lambda e^t M_X(t)[1 + \lambda e^t] 
\]

Evaluate at $t = 0$:
\[ 
    E[X^2] = M_X''(0) = \lambda(1 + \lambda) 
\]

\subsection{Variance}
\[ 
    \operatorname{Var}(X) = E[X^2] - (E[X])^2 = [\lambda(1 + \lambda)] - \lambda^2 = \lambda 
\]

Thus,
\[ 
    \boxed{E[X] = \lambda, \quad \operatorname{Var}(X) = \lambda} 
\]

\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Geometric: Mean and Variance from MGF}

\subsection{MGF}
The mean, or expected value, is given by the sum of each possible outcome multiplied by its probability:
\[
    E[X] = \sum\limits_{x=1}^\infty x \cdot p(1 - p)^{x - 1}
\]

This summation can be rewritten by factoring out $p$ and manipulating the series. 
Using the formula for the sum of an infinite geometric series and differentiation with respect to the common ratio, the expected value simplifies to:
\[
    E[X] = \sum\limits_{x=1}^\infty x \cdot p(1 - p)^{x - 1} = \dfrac{1}{p}
\]
Therefore, the mean is
\[
	\boxed{E[X] = \dfrac{1}{p}}
\]

\subsection{Moments}
The variance can be calculated using:
\[
    \operatorname{Var}(X) = E[X^2] - E[X]^2
\]
Finding the second moment, $E[X^2]$:
\[
    E[X^2] = \sum\limits_{x=1}^\infty x^2 \cdot p (1-p)^{x-1}
\]
\[
    E[X^2] = \dfrac{1-p}{p^2}
\]

\subsection{Variance}
Using the first and second moments, we can find the variance.
\[
    \operatorname{Var}(X) = \dfrac{1-p}{p^2} - (\dfrac{1}{p})^2 = \dfrac{1-p-1}{p^2} = \dfrac{-p}{p^2} = \dfrac{1-p}{p^2}
\]
Therefore, the variance is:
\[
	\boxed{\sigma^2 = \dfrac{1-p}{p^2}}
\]

\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{HyperGeometric Dist: Mean and Variance (indicator approach) }

\subsection{Setup and PMF}
Population size $N$, number of success states in the population $K$, drawn sample size $n$ (without replacement). The PMF for $X$ = number of successes in the sample is
\[
    P(X = k) = \frac{\binom{K}{k}\binom{N-K}{\,n-k\,}}{\binom{N}{n}}, \qquad
    \max(0,n-(N-K)) \le k \le \min(n,K).
\]

\subsection{Mean via indicator variables}
Label the $n$ draws and define indicator variables
\[
    I_j = \begin{cases} 1 & \text{if the $j$-th draw is a success},\\[4pt]
    0 & \text{otherwise.} \end{cases}
\]

Then $X=\sum_{j=1}^n I_j$. By linearity of expectation,
\[
    E[X] = \sum_{j=1}^n E[I_j].
\]

Since each draw (before conditioning on the others) has probability $K/N$ of being a success,
\[
    E[I_j] = \frac{K}{N} \quad\Rightarrow\quad E[X] = n\frac{K}{N}.
\]

\[
    \boxed{E[X] = n\frac{K}{N}}
\]

\subsection{Variance via covariance}
Compute variance using pairwise covariance:
\[
    \operatorname{Var}(X) = \sum_{j=1}^n \operatorname{Var}(I_j) + 2\sum_{1\le i<j\le n} \operatorname{Cov}(I_i,I_j).
\]

Let $p=K/N$. For any single indicator,
\[
    \operatorname{Var}(I_j) = p(1-p).
\]

For $i\ne j$, we need $\operatorname{Cov}(I_i,I_j)=E[I_i I_j] - p^2$.
The joint probability that both $i$ and $j$ are successes equals
\[
    P(I_i=1,I_j=1) = \frac{K}{N}\cdot\frac{K-1}{N-1} = p\cdot\frac{K-1}{N-1}.
\]

Thus
\[
    \operatorname{Cov}(I_i,I_j) = p\frac{K-1}{N-1} - p^2
    = p\left(\frac{K-1}{N-1} - p\right).
\]

Write $p = \dfrac{K}{N}$; simplifying gives the standard negative covariance:
\[
    \operatorname{Cov}(I_i,I_j) = -\frac{p(1-p)}{N-1}.
\]

Now let's calculuate the variance:
\[
    \operatorname{Var}(X) = n\cdot p(1-p) + 2\binom{n}{2}\left(-\frac{p(1-p)}{N-1}\right).
\]

Simplify the combinatorial term $2\binom{n}{2} = n(n-1)$:
\[
    \operatorname{Var}(X) = np(1-p) - \frac{n(n-1)}{N-1}p(1-p)
    = np(1-p)\left(1 - \frac{n-1}{N-1}\right).
\]

Finish simplification:
\[
    \operatorname{Var}(X) = np(1-p)\cdot\frac{N-n}{N-1}.
\]

Substitute back $p=\dfrac{K}{N}$:
\[
    \boxed{\operatorname{Var}(X) = n\frac{K}{N}\left(1-\frac{K}{N}\right)\frac{N-n}{N-1}}
\]

which is the standard hypergeometric variance formula.

\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Negative Binomial: Mean and Variance from MGF}

\subsection{First moment and Mean}
Let's start by finding the mean:
\[
	\mu_1' = \dfrac{d}{dt} M_X(t) \bigg|_{t=0} 
	= \dfrac{d}{dt} \left( \dfrac{p}{1 - qe^t}^r \right) \bigg|_{t=0}
\]
\[
	p^r \dfrac{d}{dt} (1 - qe^t) ^ {-r} \bigg|_{t=0}
\]
\[
	p^r (-r)(1-qe^t)^{-r-1} (-qe^t) \bigg|_{t=0}
\]
\[
	r (p^r) q p^{-r-1}
\]
\[
	\boxed{E[X] =  \dfrac{rq}{p}}
\]

\subsection{Second moment and Variance}
\[
	\mu_2' = \dfrac{d^2}{dt^2} M_X(t) \bigg|_{t=0} 
	= \dfrac{d	}{dt} \dfrac{d}{dt} \left( \dfrac{p}{1 - qe^t} \right) ^ r \bigg|_{t=0}
\]
\[
	r p^r \dfrac{d}{dt} \left[ (1-qe^t)^{-r-1} e^t \right] \bigg|_{t=0}
\]
\[
	r p^r q \left[ (1 - qe^t) ^ {-r-1} e^t + e^t (-r-1) (1 - qe^t)^{-r-2} \right] \bigg|_{t=0}
\]
\[
	r p^r q[ (p^{-r-1} + (r+1) p^{-r-2} q ]
\]
\[
	r p^r q p^{-r-1} + r p^r q^2 (r + 1) p ^{-r-2}
\]
\[
	\mu_2' = \dfrac{rq}{p} + r(r+1) \dfrac{q^2}{p^2} 
\]
\[
	\mu_2 = \operatorname{Var}(X) = \mu_2' - \mu_1' 
	= \dfrac{rq}{p} + r(r+1) \dfrac{q^2}{p^2} - \left( \dfrac{rq}{p} \right) ^2
\]
\[
	\dfrac{rq}{p} \left[ [1 + (r+1) \dfrac{q}{p} - \dfrac{rq}{p} \right]
\]
\[
	\dfrac{rq}{p} \left[ \dfrac{p + rq +q - rq}{p} \right]
\]
\[
	\boxed{ \operatorname{Var}(X) = \dfrac{rq}{p^2} }
\]

\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Negative HyperGeometric: Mean and Variance from MGF}
\subsection{MGF}
TODO
\subsection{Moments}
TODO
\subsection{Variance}
TODO

\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Rademacher: Mean and Variance from MGF}

The MGF for a Rademacher distrubtion is:
\[
	M(t) = E[e ^ {tX}] = \dfrac{1}{2} e ^ {-t} + \dfrac{1}{2} e ^ t = \cosh(t)
\] 
where X is a random variable with $P(X = -1) = P(X = 1) = \dfrac{1}{2}$. \\
This is because the MGF is the expectation of $e^{tX}$, and for a Rademacher variable, this expectation is calculated by summing over its possible values:
\[
	E[e^{tX}] 
		= e^{t(-1)} \cdot P(X = -1) + e^{t(1)} \cdot P(X = 1)
		= \dfrac{1}{2}e^{-t} + \dfrac{1}{2} e^{t}
\]
The MGF can also be used to find the mean and variance of the Rademacher distribution by taking its derivatives at $t = 0$.

\subsection{Deriving the MGF:}
The moment-generating function of a random variable $X$ is defined as:
$ M(t) = E[e^{tX}]$. \\ 

Apply to Rademacher distribution: For a Rademacher variable, the possible values are
$1$ and $-1$, each with a probability of $\dfrac{1}{2}$. \\ 

Calculate the expectation: Substitute these values into the definition:
\[
	M(t) = E[e ^ {tX} ] = \sum\limits_x e^{tX} P(X = x)
\]
\[
	M(t) = e^{t(-1)} P(X = -1) + e^{t(1)}P(X = 1)
\]
\[
	M(t) = e^{-t}(\dfrac{1}{2}) + e^{t}(\dfrac{1}{2})
\]
\[
	M(t) = \dfrac{1}{2} (e^{t} + e^{-t})
\]
Note that $\dfrac{e^{t} + e^{-t} }{2}$ is the definition of the hyperbolic cosine function, $\cosh(t)$. Thus, $M(t) = \cosh(t)$. \\


\subsection{Using the MGF to find moments}
\textbf{Mean}: The first derivative of the MGF evaluated at $t=0$ gives the mean.
\[
	M'(t) = \dfrac{d}{dt} \cosh(t) = \sinh(t)
\]
\[
	M'(0) = \sinh(0) = 0
\]

The mean of the Rademacher distribution is $0$.

\textbf{Variance}: The variance is found using the second derivative.
\[
	M''(t) = \dfrac{d}{dt} \sinh(t) = \cosh(t)
\]
\[
	M''(0) = \cosh(0) = 1
\]
The variance of the Rademacher distrubution is 1.

\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Zipf Distribution}

The Zipf distribution does not have a MGF, because the series for the MGF does not converge for any interval around zero. 
While it does not have a MGF, it does have a probability generating function (PGF) defined in terms of the polylogarithm function.

\begin{itemize}
    \item
    \textbf{Why the MGF does not exist:} 
        The definition of the MGF for the Zipf distribution involves an infinite series, and this series does not converge on an open interval that includes $0$. 
        For a MGF to exist, the series must converge in a neighborhood of $0$.
    \item
    \textbf{The PGF exists:}
        The Zipf distribution does have a probability generating function (PGF), which is defined as $E[z^x]$.
    \item
    \textbf{PGF Formula:}
        The PGF for the Zipf distribution is given by:
        \begin {center}
        	$ P(z) = \dfrac{Li_a(z)}{Li_a(1)} $ \quad where $Li_a(z)$ is the polylogarithm function.
        	\end{center}
        	
        
\end{itemize}

The polylogarithm function is given by:
\[
    Li_s(z) = \sum\limits_{n=1}^\infty \dfrac{z^n}{n^s}
\]
\[
    Li_s(z) = \dfrac{1}{ \Gamma(s)} \int_0^\infty \dfrac{t^{s - 1}}{\frac{e^t}{z} - 1} dt
\]

The Zipf distribution is used often, particularly for analyzing patterns in fields like linguistics, internet traffic, city populations, and economics, where a few high-frequency items dominate a much larger number of low-frequency ones. It's a fundamental concept for understanding how rank and frequency relate in many natural and human-made systems.

\pagebreak


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exponential Distribution: Mean and Variance from MGF}

\subsection{PDF and MGF}
The exponential distribution (rate parameter $\lambda>0$) has PDF
\[
    f_X(x) = \lambda e^{-\lambda x}, \qquad x \ge 0.
\]

Its moment generating function (for $t < \lambda$) is
\[
    M_X(t) = E[e^{tX}] = \int_0^\infty e^{tx}\lambda e^{-\lambda x}\,dx
    = \lambda \int_0^\infty e^{-(\lambda - t)x}\,dx.
\]

Evaluating the integral (geometric/exponential integral):
\[
    M_X(t) = \lambda \left[\frac{e^{-(\lambda - t)x}}{-(\lambda - t)}\right]_{0}^{\infty}
    = \lambda \left( 0 - \frac{1}{-(\lambda - t)} \right)
    = \frac{\lambda}{\lambda - t}.
\]

\subsection{Raw moments and variance}
Differentiate the MGF:
\[
    M_X'(t) = \frac{d}{dt}\left(\frac{\lambda}{\lambda - t}\right)
    = \frac{\lambda}{(\lambda - t)^2},
    \qquad
    M_X''(t) = \frac{2\lambda}{(\lambda - t)^3}.
\]

Evaluate at $t=0$ to get raw moments:
\[
    E[X] = M_X'(0) = \frac{\lambda}{\lambda^2} = \frac{1}{\lambda},
    \qquad
    E[X^2] = M_X''(0) = \frac{2\lambda}{\lambda^3} = \frac{2}{\lambda^2}.
\]

Variance:
\[
    \operatorname{Var}(X) = E[X^2] - (E[X])^2
    = \frac{2}{\lambda^2} - \frac{1}{\lambda^2}
    = \frac{1}{\lambda^2}.
\]
\[
    \boxed{E[X] = \frac{1}{\lambda}, \qquad \operatorname{Var}(X) = \frac{1}{\lambda^2}}
\]

% \pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

