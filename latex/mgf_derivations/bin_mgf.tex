\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{multicol}
\usepackage{enumitem}
\usepackage{titlesec}
\usepackage{helvet}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{hyperref}
\titleformat{\section}{\large\bfseries}{}{0em}{}[\titlerule]
\titlespacing{\section}{0pt}{0.5em}{0.25em}
\setlength{\parskip}{0.2em}
\setlength{\parindent}{0em}
\setlist[itemize]{noitemsep, topsep=0pt, left=1em}
\usepackage[none]{hyphenat}
\pagenumbering{gobble}
\usepackage{amsmath}

\vspace*{-2cm}
\begin{document}

\section{Binomial: Mean and Variance from MGF}

\subsection{MGF of Binomial Distribution}
The moment generating function (MGF) of a binomial distribution is given by:
\[
    M_X(t) = E[e^{tX}] = \sum_{x=0}^{n} e^{tx} P(X = x)
\]

We know that
\[
    P(X = x) = \binom{n}{x} p^x q^{n-x}, \quad \text{where } q = 1 - p
\]
Substituting in:
\[
    M_X(t) = \sum_{x=0}^{n} \binom{n}{x} (pe^t)^x q^{n-x}
\]
By the Binomial Theorem, this simplifies to:
\[
    M_X(t) = (q + pe^t)^n
\]

\subsection{Raw Moments}
We can find the first and second raw moments by differentiating the MGF.

\textbf{First moment (mean):}
\[
    \mu_1' = E[X] = \frac{dM_X(t)}{dt}\bigg|_{t=0}
\]
Differentiate:
\[
    \frac{dM_X(t)}{dt} = n(q + pe^t)^{n-1}(pe^t)
\]
Now evaluate at $t = 0$:
\[
    \mu_1' = np(q + p)^{n-1} = np(1)^{n-1} = np
\]
Therefore, the mean is:
\[
    \boxed{E[X] = np}
\]

\textbf{Second moment:}
\[
    \mu_2' = \frac{d^2 M_X(t)}{dt^2}\bigg|_{t=0}
\]
Differentiate again:
\[
    \frac{d^2 M_X(t)}{dt^2} = np \left[ e^t (n-1)(q + pe^t)^{n-2}(pe^t) + e^t (q + pe^t)^{n-1} \right]
\]
Simplify and evaluate at $t = 0$:
\[
    \mu_2' = np \left[ (n-1)p + 1 \right] = n(n-1)p^2 + np
\]

\subsection{Variance of Binomial Distribution}
The variance is given by:
\[
    \sigma^2 = E[X^2] - (E[X])^2 = \mu_2' - (\mu_1')^2
\]
Substitute the results:
\[
    \sigma^2 = \left[ n(n-1)p^2 + np \right] - (np)^2
\]
Simplify:
\[
    \sigma^2 = n(n-1)p^2 + np - n^2p^2 = np(1 - p)
\]
Thus,
\[
    \boxed{\sigma^2 = Var(X) = npq}
\]
\pagebreak 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bernoulli: Mean and Variance from MGF}
\subsection{MGF of Bernoulli Distribution}
Given the moment-generating function (MGF):
\[
    M_X(t) = 1 - p + pe^t 
\]

\subsection{Raw Moments}
Let's find the first moment:
\[
    \dfrac{d}{dt} M_X(t) = \dfrac{d}{dt} (1 - p + pe^t) = pe^t
\]
\[
 	M_X'(0) = pe^0 
\]
Therefore, the mean is:
\[
    \boxed{E[X] = p}
\]

Let's find the second moment:
\[
    \dfrac{d^2}{dt^2} M_X(t) = \dfrac{d}{dt} (pe^t) = pe^t
\]

\subsection{Variance of Bernoulli Distribution}
Let's find the variance: $\sigma^2 = E[X^2] - E[X]^2$. Substituting yields: \\
\[
    \sigma^2 = p - p^2 = p(1 - p) = pq
\]
Therefore, the variance is: \\
\[
	\boxed{\sigma^2 = pq}
\]

\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Poisson: Mean and Variance from MGF}
\subsection{MGF of Poisson Distribution}

The Poisson distribution is discrete, with PMF:
\[ 
    P(X = x) = \frac{e^{-\lambda}\lambda^x}{x!}, \quad x = 0, 1, 2, \ldots 
\]

The moment generating function (MGF) is:
\[ 
    M_X(t) = E[e^{tX}] = \sum_{x=0}^{\infty} e^{tx} P(X = x) 
\]

Substitute the PMF:
\[ 
    M_X(t) = e^{-\lambda} \sum_{x=0}^{\infty} \frac{(\lambda e^t)^x}{x!} 
\]

Recognize this as the Taylor expansion of $e^{z}$:
\[ 
    \sum_{x=0}^{\infty} \frac{z^x}{x!} = e^z 
\]

Thus,
\[
    M_X(t) = e^{-\lambda} \cdot e^{\lambda e^t} = e^{\lambda(e^t - 1)} 
\]

\subsection{Raw Moments}
The mean and variance can be derived from derivatives of $M_X(t)$.

\textbf{First moment (mean):}
\[ 
    E[X] = M_X'(0) 
\]

Differentiate:
\[ 
    M_X'(t) = \lambda e^t e^{\lambda(e^t - 1)} = \lambda e^t M_X(t) 
\]

Evaluate at $t = 0$:
\[  
    E[X] = M_X'(0) = \lambda e^0 M_X(0) = \lambda 
\]

\textbf{Second moment:}
\[ 
    M_X''(t) = \frac{d}{dt}[\lambda e^t M_X(t)] = \lambda e^t M_X(t) + \lambda e^t M_X'(t) 
\]

Substitute $M_X'(t) = \lambda e^t M_X(t)$:
\[ 
    M_X''(t) = \lambda e^t M_X(t)[1 + \lambda e^t] 
\]

Evaluate at $t = 0$:
\[ 
    E[X^2] = M_X''(0) = \lambda(1 + \lambda) 
\]

\subsection{Variance}
\[ 
    Var(X) = E[X^2] - (E[X])^2 = [\lambda(1 + \lambda)] - \lambda^2 = \lambda 
\]

Thus,
\[ 
    \boxed{E[X] = \lambda, \quad Var(X) = \lambda} 
\]

\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Geometric: Mean and Variance from MGF}

\subsection{MGF}
TODO
\subsection{Moments}
TODO
\subsection{Variance}
TODO

\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{HyperGeometric: Mean and Variance from MGF}
\subsection{MGF}
TODO
\subsection{Moments}
TODO
\subsection{Variance}
TODO

\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Negative Binomial: Mean and Variance from MGF}
\subsection{MGF}
TODO
\subsection{Moments}
TODO
\subsection{Variance}
TODO

\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Negative HyperGeometric: Mean and Variance from MGF}
\subsection{MGF}
TODO
\subsection{Moments}
TODO
\subsection{Variance}
TODO

\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Rademacher: Mean and Variance from MGF}

The MGF for a Rademacher distrubtion is:
\[
	M(t) = E[e ^ {tX}] = \dfrac{1}{2} e ^ {-t} + \dfrac{1}{2} e ^ t = \cosh(t)
\] 
where X is a random variable with $P(X = -1) = P(X = 1) = \dfrac{1}{2}$. \\
This is because the MGF is the expectation of $e^{tX}$, and for a Rademacher variable, this expectation is calculated by summing over its possible values:
\[
	E[e^{tX}] 
		= e^{t(-1)} \cdot P(X = -1) + e^{t(1)} \cdot P(X = 1)
		= \dfrac{1}{2}e^{-t} + \dfrac{1}{2} e^{t}
\]
The MGF can also be used to find the mean and variance of the Rademacher distribution by taking its derivatives at $t = 0$.

\subsection{Deriving the MGF:}
The moment-generating function of a random variable $X$ is defined as:
$ M(t) = E[e^{tX}]$. \\ 

Apply to Rademacher distribution: For a Rademacher variable, the possible values are
$1$ and $-1$, each with a probability of $\dfrac{1}{2}$. \\ 

Calculate the expectation: Substitute these values into the definition:
\[
	M(t) = E[e ^ {tX} ] = \sum\limits_x e^{tX} P(X = x)
\]
\[
	M(t) = e^{t(-1)} P(X = -1) + e^{t(1)}P(X = 1)
\]
\[
	M(t) = e^{-t}(\dfrac{1}{2}) + e^{t}(\dfrac{1}{2})
\]
\[
	M(t) = \dfrac{1}{2} (e^{t} + e^{-1})
\]
Note that $\dfrac{e^{t} + e^{-1} }{2}$ is the definition of the hyperbolic cosine function, $\cosh(t)$. Thus, $M(t) = \cosh(t)$. \\


\subsection{Using the MGF to find moments}
\textbf{Mean}: The first derivative of the MGF evaluated at $t=0$ gives the mean.
\[
	M'(t) = \dfrac{d}{dt} \cosh(t) = \sinh(t)
\]
\[
	M'(0) = \sinh(0) = 0
\]
The mean of the Rademacher distribution is $0$.

\textbf{Variance}: The variance is found using the second derivative.
\[
	M''(t) = \dfrac{d}{dt} \sinh(t) = \cosh(t)
\]
\[
	M''(0) = \cosh(0) = 1
\]
The variance of the Rademacher distrubution is 1.

\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Zipf Distribution}

The Zipf distribution does not have a MGF, because the series for the MGF does not converge for any interval around zero. 
While it does not have a MGF, it does have a probability generating function (PGF) defined in terms of the polylogarithm function.

\begin{itemize}
    \item
    \textbf{Why the MGF does not exist:} 
        The definition of the MGF for the Zipf distribution involves an infinite series, and this series does not converge on an open interval that includes $0$. 
        For a MGF to exist, the series must converge in a neighborhood of $0$.
    \item
    \textbf{The PGF exists:}
        The Zipf distribution does have a probability generating function (PGF), which is defined as $E[z^x]$.
    \item
    \textbf{PGF Formula:}
        The PGF for the Zipf distribution is given by:
        \begin {center}
        	$ P(z) = \dfrac{Li_a(z)}{Li_a(1)} $ \quad where $Li_a(z)$ is the polylogarithm function.
        	\end{center}
        	
        
\end{itemize}

The polylogarithm function is given by:
\[
    Li_s(z) = \sum\limits_{n=1}^\infty \dfrac{z^n}{n^s}
\]
\[
    Li_s(z) = \dfrac{1}{ \Gamma(s)} \int_0^\infty \dfrac{t^{s - 1}}{\frac{e^t}{z} - 1} dt
\]

The Zipf distribution is used often, particularly for analyzing patterns in fields like linguistics, internet traffic, city populations, and economics, where a few high-frequency items dominate a much larger number of low-frequency ones. It's a fundamental concept for understanding how rank and frequency relate in many natural and human-made systems.

\pagebreak


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exponential Distribution}
The exponential distribution can be thought of as a continuous analogue of the Poisson process.

\subsection{MGF of Exponential Distribution}
The moment generating function of the Exponential distribution is given by:
\[
    M_X(t) = E(e^{tx}) = \int_{-\infty}^\infty e^{tx} f_X(x) dx
\]
\[
    M_X(t) = \int_0^\infty e^{tx} \lambda e ^ {-\lambda x } dx 
\]
\[
    M_X(t) = \lambda \int_0^\infty e^{tx - \lambda x} dx 
        = \lambda \int_0^\infty e^{x ( t - \lambda) } dx 
\]
\[
    M_X(t) = \lambda \dfrac{e^{x(t - \lambda)}}{(t - \lambda)} \bigg|_{0}^\infty
\]
\[
    M_X(t) = \lambda \dfrac{0 -1}{(t - \lambda)}
\]
Thus, the general form of the moment generating function, in terms of $\lambda$, is:
\[
    M_X(t) = \dfrac{ \lambda } { \lambda - t }
\]

\subsection{Raw Moments}
TODO 

\subsection{Variance}
TODO 

% \pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}

