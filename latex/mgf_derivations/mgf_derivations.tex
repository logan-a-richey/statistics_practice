\documentclass[10pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{multicol}
\usepackage{enumitem}
\usepackage{titlesec}
\usepackage{helvet}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{hyperref}
\titleformat{\section}{\large\bfseries}{}{0em}{}[\titlerule]
\titlespacing{\section}{0pt}{0.5em}{0.25em}
\setlength{\parskip}{0.2em}
\setlength{\parindent}{0em}
\setlist[itemize]{noitemsep, topsep=0pt, left=1em}
\usepackage[none]{hyphenat}
\pagenumbering{gobble}
\usepackage{amsmath}

\vspace*{-2cm}
\begin{document}

\section{Binomial: Mean and Variance from MGF}

\subsection{MGF of Binomial Distribution}
The moment generating function (MGF) of a binomial distribution is given by:
\[
    M_X(t) = E[e^{tX}] = \sum_{x=0}^{n} e^{tx} P(X = x)
\]

We know that
\[
    P(X = x) = \binom{n}{x} p^x q^{n-x}, \quad \text{where } q = 1 - p
\]
Substituting in:
\[
    M_X(t) = \sum_{x=0}^{n} \binom{n}{x} (pe^t)^x q^{n-x}
\]
By the Binomial Theorem, this simplifies to:
\[
    M_X(t) = (q + pe^t)^n
\]

\subsection{Raw Moments}
We can find the first and second raw moments by differentiating the MGF.

\textbf{First moment (mean):}
\[
    \mu_1' = E[X] = \frac{dM_X(t)}{dt}\bigg|_{t=0}
\]
Differentiate:
\[
    \frac{dM_X(t)}{dt} = n(q + pe^t)^{n-1}(pe^t)
\]
Now evaluate at $t = 0$:
\[
    \mu_1' = np(q + p)^{n-1} = np(1)^{n-1} = np
\]
Therefore, the mean is:
\[
    \boxed{E[X] = np}
\]

\textbf{Second moment:}
\[
    \mu_2' = \frac{d^2 M_X(t)}{dt^2}\bigg|_{t=0}
\]
Differentiate again:
\[
    \frac{d^2 M_X(t)}{dt^2} = np \left[ e^t (n-1)(q + pe^t)^{n-2}(pe^t) + e^t (q + pe^t)^{n-1} \right]
\]
Simplify and evaluate at $t = 0$:
\[
    \mu_2' = np \left[ (n-1)p + 1 \right] = n(n-1)p^2 + np
\]

\subsection{Variance of Binomial Distribution}
The variance is given by:
\[
    \sigma^2 = E[X^2] - (E[X])^2 = \mu_2' - (\mu_1')^2
\]
Substitute the results:
\[
    \sigma^2 = \left[ n(n-1)p^2 + np \right] - (np)^2
\]
Simplify:
\[
    \sigma^2 = n(n-1)p^2 + np - n^2p^2 = np(1 - p)
\]
Thus,
\[
    \boxed{\sigma^2 = \operatorname{Var}(X) = npq}
\]
\pagebreak 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bernoulli: Mean and Variance from MGF}
\subsection{MGF of Bernoulli Distribution}
Given the moment-generating function (MGF):
\[
    M_X(t) = 1 - p + pe^t 
\]

\subsection{Raw Moments}
Let's find the first moment:
\[
    \dfrac{d}{dt} M_X(t) = \dfrac{d}{dt} (1 - p + pe^t) = pe^t
\]
\[
 	M_X'(0) = pe^0 
\]
Therefore, the mean is:
\[
    \boxed{E[X] = p}
\]

Let's find the second moment:
\[
    \dfrac{d^2}{dt^2} M_X(t) = \dfrac{d}{dt} (pe^t) = pe^t
\]

\subsection{Variance of Bernoulli Distribution}
Let's find the variance: $\sigma^2 = E[X^2] - E[X]^2$. Substituting yields: \\
\[
    \sigma^2 = p - p^2 = p(1 - p) = pq
\]
Therefore, the variance is: \\
\[
	\boxed{\sigma^2 = pq}
\]

\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Poisson: Mean and Variance from MGF}
\subsection{MGF of Poisson Distribution}

The Poisson distribution is discrete, with PMF:
\[ 
    P(X = x) = \frac{e^{-\lambda}\lambda^x}{x!}, \quad x = 0, 1, 2, \ldots 
\]

The moment generating function (MGF) is:
\[ 
    M_X(t) = E[e^{tX}] = \sum_{x=0}^{\infty} e^{tx} P(X = x) 
\]

Substitute the PMF:
\[ 
    M_X(t) = e^{-\lambda} \sum_{x=0}^{\infty} \frac{(\lambda e^t)^x}{x!} 
\]

Recognize this as the Taylor expansion of $e^{z}$:
\[ 
    \sum_{x=0}^{\infty} \frac{z^x}{x!} = e^z 
\]

Thus,
\[
    M_X(t) = e^{-\lambda} \cdot e^{\lambda e^t} = e^{\lambda(e^t - 1)} 
\]

\subsection{Raw Moments}
The mean and variance can be derived from derivatives of $M_X(t)$.

\textbf{First moment (mean):}
\[ 
    E[X] = M_X'(0) 
\]

Differentiate:
\[ 
    M_X'(t) = \lambda e^t e^{\lambda(e^t - 1)} = \lambda e^t M_X(t) 
\]

Evaluate at $t = 0$:
\[  
    E[X] = M_X'(0) = \lambda e^0 M_X(0) = \lambda 
\]

\textbf{Second moment:}
\[ 
    M_X''(t) = \frac{d}{dt}[\lambda e^t M_X(t)] = \lambda e^t M_X(t) + \lambda e^t M_X'(t) 
\]

Substitute $M_X'(t) = \lambda e^t M_X(t)$:
\[ 
    M_X''(t) = \lambda e^t M_X(t)[1 + \lambda e^t] 
\]

Evaluate at $t = 0$:
\[ 
    E[X^2] = M_X''(0) = \lambda(1 + \lambda) 
\]

\subsection{Variance}
\[ 
    \operatorname{Var}(X) = E[X^2] - (E[X])^2 = [\lambda(1 + \lambda)] - \lambda^2 = \lambda 
\]

Thus,
\[ 
    \boxed{E[X] = \lambda, \quad \operatorname{Var}(X) = \lambda} 
\]

\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Geometric: Mean and Variance from MGF}

\subsection{MGF}
The mean, or expected value, is given by the sum of each possible outcome multiplied by its probability:
\[
    E[X] = \sum\limits_{x=1}^\infty x \cdot p(1 - p)^{x - 1}
\]

This summation can be rewritten by factoring out $p$ and manipulating the series. 
Using the formula for the sum of an infinite geometric series and differentiation with respect to the common ratio, the expected value simplifies to:
\[
    E[X] = \sum\limits_{x=1}^\infty x \cdot p(1 - p)^{x - 1} = \dfrac{1}{p}
\]
Therefore, the mean is
\[
	\boxed{E[X] = \dfrac{1}{p}}
\]

\subsection{Moments}
The variance can be calculated using:
\[
    \operatorname{Var}(X) = E[X^2] - E[X]^2
\]
Finding the second moment, $E[X^2]$:
\[
    E[X^2] = \sum\limits_{x=1}^\infty x^2 \cdot p (1-p)^{x-1}
\]
\[
    E[X^2] = \dfrac{1-p}{p^2}
\]

\subsection{Variance}
Using the first and second moments, we can find the variance.
\[
    \operatorname{Var}(X) = \dfrac{1-p}{p^2} - (\dfrac{1}{p})^2 = \dfrac{1-p-1}{p^2} = \dfrac{-p}{p^2} = \dfrac{1-p}{p^2}
\]
Therefore, the variance is:
\[
	\boxed{\sigma^2 = \dfrac{1-p}{p^2}}
\]

\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{HyperGeometric Dist: Mean and Variance (indicator approach) }

\subsection{Setup and PMF}
Population size $N$, number of success states in the population $K$, drawn sample size $n$ (without replacement). The PMF for $X$ = number of successes in the sample is
\[
    P(X = k) = \frac{\binom{K}{k}\binom{N-K}{\,n-k\,}}{\binom{N}{n}}, \qquad
    \max(0,n-(N-K)) \le k \le \min(n,K).
\]

\subsection{Mean via indicator variables}
Label the $n$ draws and define indicator variables
\[
    I_j = \begin{cases} 1 & \text{if the $j$-th draw is a success},\\[4pt]
    0 & \text{otherwise.} \end{cases}
\]

Then $X=\sum_{j=1}^n I_j$. By linearity of expectation,
\[
    E[X] = \sum_{j=1}^n E[I_j].
\]

Since each draw (before conditioning on the others) has probability $K/N$ of being a success,
\[
    E[I_j] = \frac{K}{N} \quad\Rightarrow\quad E[X] = n\frac{K}{N}.
\]

\[
    \boxed{E[X] = n\frac{K}{N}}
\]

\subsection{Variance via covariance}
Compute variance using pairwise covariance:
\[
    \operatorname{Var}(X) = \sum_{j=1}^n \operatorname{Var}(I_j) + 2\sum_{1\le i<j\le n} \operatorname{Cov}(I_i,I_j).
\]

Let $p=K/N$. For any single indicator,
\[
    \operatorname{Var}(I_j) = p(1-p).
\]

For $i\ne j$, we need $\operatorname{Cov}(I_i,I_j)=E[I_i I_j] - p^2$.
The joint probability that both $i$ and $j$ are successes equals
\[
    P(I_i=1,I_j=1) = \frac{K}{N}\cdot\frac{K-1}{N-1} = p\cdot\frac{K-1}{N-1}.
\]

Thus
\[
    \operatorname{Cov}(I_i,I_j) = p\frac{K-1}{N-1} - p^2
    = p\left(\frac{K-1}{N-1} - p\right).
\]

Write $p = \dfrac{K}{N}$; simplifying gives the standard negative covariance:
\[
    \operatorname{Cov}(I_i,I_j) = -\frac{p(1-p)}{N-1}.
\]

Now let's calculuate the variance:
\[
    \operatorname{Var}(X) = n\cdot p(1-p) + 2\binom{n}{2}\left(-\frac{p(1-p)}{N-1}\right).
\]

Simplify the combinatorial term $2\binom{n}{2} = n(n-1)$:
\[
    \operatorname{Var}(X) = np(1-p) - \frac{n(n-1)}{N-1}p(1-p)
    = np(1-p)\left(1 - \frac{n-1}{N-1}\right).
\]

Finish simplification:
\[
    \operatorname{Var}(X) = np(1-p)\cdot\frac{N-n}{N-1}.
\]

Substitute back $p=\dfrac{K}{N}$:
\[
    \boxed{\operatorname{Var}(X) = n\frac{K}{N}\left(1-\frac{K}{N}\right)\frac{N-n}{N-1}}
\]

which is the standard hypergeometric variance formula.

\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Negative Binomial: Mean and Variance from MGF}

% \subsection{First moment and Mean}
% Let's start by finding the mean:
% \[
% 	\mu_1' = \dfrac{d}{dt} M_X(t) \bigg|_{t=0} 
% 	= \dfrac{d}{dt} \left( \dfrac{p}{1 - qe^t}^r \right) \bigg|_{t=0}
% \]
% \[
% 	p^r \dfrac{d}{dt} (1 - qe^t) ^ {-r} \bigg|_{t=0}
% \]
% \[
% 	p^r (-r)(1-qe^t)^{-r-1} (-qe^t) \bigg|_{t=0}
% \]
% \[
% 	r (p^r) q p^{-r-1}
% \]
% \[
% 	\boxed{E[X] =  \dfrac{rq}{p}}
% \]
% 
% \subsection{Second moment and Variance}
% \[
% 	\mu_2' = \dfrac{d^2}{dt^2} M_X(t) \bigg|_{t=0} 
% 	= \dfrac{d	}{dt} \dfrac{d}{dt} \left( \dfrac{p}{1 - qe^t} \right) ^ r \bigg|_{t=0}
% \]
% \[
% 	r p^r \dfrac{d}{dt} \left[ (1-qe^t)^{-r-1} e^t \right] \bigg|_{t=0}
% \]
% \[
% 	r p^r q \left[ (1 - qe^t) ^ {-r-1} e^t + e^t (-r-1) (1 - qe^t)^{-r-2} \right] \bigg|_{t=0}
% \]
% \[
% 	r p^r q[ (p^{-r-1} + (r+1) p^{-r-2} q ]
% \]
% \[
% 	r p^r q p^{-r-1} + r p^r q^2 (r + 1) p ^{-r-2}
% \]
% \[
% 	\mu_2' = \dfrac{rq}{p} + r(r+1) \dfrac{q^2}{p^2} 
% \]
% \[
% 	\mu_2 = \operatorname{Var}(X) = \mu_2' - \mu_1' 
% 	= \dfrac{rq}{p} + r(r+1) \dfrac{q^2}{p^2} - \left( \dfrac{rq}{p} \right) ^2
% \]
% \[
% 	\dfrac{rq}{p} \left[ [1 + (r+1) \dfrac{q}{p} - \dfrac{rq}{p} \right]
% \]
% \[
% 	\dfrac{rq}{p} \left[ \dfrac{p + rq +q - rq}{p} \right]
% \]
% \[
% 	\boxed{ \operatorname{Var}(X) = \dfrac{rq}{p^2} }
% \]

% Attempt 2:
% Parameterization: X = number of failures before the r-th success.
% PMF: P(X=k) = \binom{k+r-1}{k} p^r q^k, k = 0,1,2,..., q=1-p.

\subsection{MGF and first moment}
The MGF for the negative binomial (counting failures before the \(r\)-th success,
with success probability \(p\) and \(q=1-p\)) is
\[
    M_X(t) = \left(\frac{p}{1-qe^t}\right)^r
    = p^r(1-qe^t)^{-r}, \qquad \text{for } |qe^t|<1.
\]

A compact way to differentiate is to write
\[
    \ln M_X(t) = r\ln p - r\ln(1-qe^t).
\]

Then
\[
    \frac{M_X'(t)}{M_X(t)} = \frac{d}{dt}\ln M_X(t) = -r\cdot\frac{d}{dt}\ln(1-qe^t)
    = -r\cdot\frac{-q e^t}{1-qe^t}
    = \frac{rq e^t}{1-qe^t}.
\]

Hence
\[
    M_X'(t) = M_X(t)\cdot\frac{rq e^t}{1-qe^t}.
\]
    Evaluate at \(t=0\). Since \(M_X(0)=1\) and \(e^0=1\),
\[
    E[X]=M_X'(0)=\frac{rq}{1-q}=\frac{rq}{p}.
\]
\[
    \boxed{E[X]=\dfrac{rq}{p}}
\]

\subsection{Second moment and variance}
Differentiate again. Let
\[
    A(t)=\frac{rq e^t}{1-qe^t},\qquad M_X'(t)=M_X(t)A(t).
\]

Then
\[
    M_X''(t)=\frac{d}{dt}\big(M_X(t)A(t)\big)=M_X(t)A(t)^2 + M_X(t)A'(t).
\]

So
\[
    M_X''(0)=A(0)^2 + A'(0),
\]

because \(M_X(0)=1\). Compute
\[
    A(0)=\frac{rq}{1-q}=\frac{rq}{p},
    \qquad
    A'(t)=\frac{rq e^t}{(1-qe^t)^2},
\]

thus
\[
    A'(0)=\frac{rq}{(1-q)^2}=\frac{rq}{p^2}.
\]

Therefore
\[
    E[X^2]=M_X''(0)=\left(\frac{rq}{p}\right)^2 + \frac{rq}{p^2}
    = \frac{rq(rq+1)}{p^2}.
\]

Now compute the variance:
\[
    \operatorname{Var}(X)=E[X^2]-(E[X])^2
    = \frac{rq(rq+1)}{p^2} - \left(\frac{rq}{p}\right)^2
    = \frac{rq}{p^2}.
\]
\[
    \boxed{\operatorname{Var}(X)=\dfrac{rq}{p^2}}
\]

\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Negative Hypergeometric (Failures before the $r$-th success)}

% Model: Population of size N, containing K successes and N-K failures.
% Draw without replacement until the r-th success is observed.
% Let X = number of failures observed before the r-th success.
% Support: X = 0,1,2,...,N-K (but truncated by feasibility when r > K, etc.)

\subsection{Setup and goal}
Population size \(N\), successes \(K\), failures \(N-K\). We draw items without replacement until we have observed \(r\) successes (\(1 \le r \le K\)). Let
\[
X = \text{number of failures observed before the \(r\)-th success.}
\]
We derive \(E[X]\) and \(\operatorname{Var}(X)\) using indicator variables.

\subsection{Indicator decomposition}
Label the \(N-K\) failures in the population and for each failure \(j\) define the indicator
\[
I_j = \begin{cases}
1 & \text{if failure } j \text{ is drawn before the \(r\)-th success},\\[4pt]
0 & \text{otherwise.}
\end{cases}
\]
Then
\[
X=\sum_{j=1}^{\,N-K} I_j.
\]
By linearity,
\[
E[X] = \sum_{j=1}^{N-K} E[I_j] = (N-K)\,P(\text{a fixed failure is before the \(r\)-th success}).
\]

\subsection{Single-indicator probability}
Consider one fixed failure and the \(K\) successes. In a random ordering of these \(K+1\) items the failure is equally likely to occupy any of the \(K+1\) positions. The failure appears before the \(r\)-th success iff its position among these \(K+1\) items is \(\le r\). Hence
\[
P(I_j=1) = \frac{r}{K+1}.
\]
Therefore
\[
\boxed{E[X] = (N-K)\frac{r}{K+1} = \dfrac{r(N-K)}{K+1}.}
\]

\pagebreak

\subsection{Pair probability and covariance}
For two distinct failures \(i\neq j\) consider these two failures together with the \(K\) successes (total \(K+2\) items). Both failures are before the \(r\)-th success exactly when, among the \(K+2\) items, the later failure (the one with larger position) lies at position \(\le r+1\). Counting admissible placements gives the closed form
\[
P(I_i=1,I_j=1) = \frac{r(r+1)}{(K+1)(K+2)}.
\]
Let \(p:=P(I_j=1)=\dfrac{r}{K+1}\). Then the covariance between two distinct failure indicators is
\[
\operatorname{Cov}(I_i,I_j)
= P(I_i=1,I_j=1) - p^2
= \frac{r(r+1)}{(K+1)(K+2)} - \frac{r^2}{(K+1)^2}
= \frac{r(K-r+1)}{(K+1)^2(K+2)}.
\]

\subsection{Assemble the variance}
Use
\[
\operatorname{Var}(X) = \sum_{j=1}^{N-K}\operatorname{Var}(I_j) + 2\sum_{1\le i<j\le N-K}\operatorname{Cov}(I_i,I_j).
\]
We have \(\operatorname{Var}(I_j)=p(1-p)\) and there are \(\binom{N-K}{2}\) equal covariance terms. Substitute and simplify:
\[
\begin{aligned}
\operatorname{Var}(X)
&= (N-K)\,p(1-p) + 2\binom{N-K}{2}\,\operatorname{Cov}(I_i,I_j)\\[6pt]
&= (N-K)\frac{r}{K+1}\!\left(1-\frac{r}{K+1}\right)
+ (N-K)(N-K-1)\frac{r(K-r+1)}{(K+1)^2(K+2)}.
\end{aligned}
\]
This simplifies (after algebra) to the common closed form
\[
\boxed{\operatorname{Var}(X)
= \dfrac{r (N-K)(N+1)(K-r+1)}{(K+1)^2 (K+2)}.}
\]

\subsection{Final Results}
\[
\boxed{E[X]=\dfrac{r(N-K)}{K+1}, \qquad 
\operatorname{Var}(X)=\dfrac{r (N-K)(N+1)(K-r+1)}{(K+1)^2 (K+2)}.}
\]

% Remark: Another useful quantity is T = total draws needed to get r successes: T = X + r.
% Then E[T] = r\frac{N+1}{K+1} and Var(T) = Var(X) (same variance as X).

\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Rademacher: Mean and Variance from MGF}

The MGF for a Rademacher distrubtion is:
\[
	M(t) = E[e ^ {tX}] = \dfrac{1}{2} e ^ {-t} + \dfrac{1}{2} e ^ t = \cosh(t)
\] 
where X is a random variable with $P(X = -1) = P(X = 1) = \dfrac{1}{2}$. \\
This is because the MGF is the expectation of $e^{tX}$, and for a Rademacher variable, this expectation is calculated by summing over its possible values:
\[
	E[e^{tX}] 
		= e^{t(-1)} \cdot P(X = -1) + e^{t(1)} \cdot P(X = 1)
		= \dfrac{1}{2}e^{-t} + \dfrac{1}{2} e^{t}
\]
The MGF can also be used to find the mean and variance of the Rademacher distribution by taking its derivatives at $t = 0$.

\subsection{Deriving the MGF:}
The moment-generating function of a random variable $X$ is defined as:
$ M(t) = E[e^{tX}]$. \\ 

Apply to Rademacher distribution: For a Rademacher variable, the possible values are
$1$ and $-1$, each with a probability of $\dfrac{1}{2}$. \\ 

Calculate the expectation: Substitute these values into the definition:
\[
	M(t) = E[e ^ {tX} ] = \sum\limits_x e^{tX} P(X = x)
\]
\[
	M(t) = e^{t(-1)} P(X = -1) + e^{t(1)}P(X = 1)
\]
\[
	M(t) = e^{-t}(\dfrac{1}{2}) + e^{t}(\dfrac{1}{2})
\]
\[
	M(t) = \dfrac{1}{2} (e^{t} + e^{-t})
\]
Note that $\dfrac{e^{t} + e^{-t} }{2}$ is the definition of the hyperbolic cosine function, $\cosh(t)$. Thus, $M(t) = \cosh(t)$. \\


\subsection{Using the MGF to find moments}
\textbf{Mean}: The first derivative of the MGF evaluated at $t=0$ gives the mean.
\[
	M'(t) = \dfrac{d}{dt} \cosh(t) = \sinh(t)
\]
\[
	M'(0) = \sinh(0) = 0
\]

The mean of the Rademacher distribution is $0$.

\textbf{Variance}: The variance is found using the second derivative.
\[
	M''(t) = \dfrac{d}{dt} \sinh(t) = \cosh(t)
\]
\[
	M''(0) = \cosh(0) = 1
\]
The variance of the Rademacher distrubution is 1.

\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exponential Distribution: Mean and Variance from MGF}

\subsection{PDF and MGF}
The exponential distribution (rate parameter $\lambda>0$) has PDF
\[
    f_X(x) = \lambda e^{-\lambda x}, \qquad x \ge 0.
\]

Its moment generating function (for $t < \lambda$) is
\[
    M_X(t) = E[e^{tX}] = \int_0^\infty e^{tx}\lambda e^{-\lambda x}\,dx
    = \lambda \int_0^\infty e^{-(\lambda - t)x}\,dx.
\]

Evaluating the integral (geometric/exponential integral):
\[
    M_X(t) = \lambda \left[\frac{e^{-(\lambda - t)x}}{-(\lambda - t)}\right]_{0}^{\infty}
    = \lambda \left( 0 - \frac{1}{-(\lambda - t)} \right)
    = \frac{\lambda}{\lambda - t}.
\]

\subsection{Raw moments and variance}
Differentiate the MGF:
\[
    M_X'(t) = \frac{d}{dt}\left(\frac{\lambda}{\lambda - t}\right)
    = \frac{\lambda}{(\lambda - t)^2},
    \qquad
    M_X''(t) = \frac{2\lambda}{(\lambda - t)^3}.
\]

Evaluate at $t=0$ to get raw moments:
\[
    E[X] = M_X'(0) = \frac{\lambda}{\lambda^2} = \frac{1}{\lambda},
    \qquad
    E[X^2] = M_X''(0) = \frac{2\lambda}{\lambda^3} = \frac{2}{\lambda^2}.
\]

Variance:
\[
    \operatorname{Var}(X) = E[X^2] - (E[X])^2
    = \frac{2}{\lambda^2} - \frac{1}{\lambda^2}
    = \frac{1}{\lambda^2}.
\]
\[
    \boxed{E[X] = \frac{1}{\lambda}, \qquad \operatorname{Var}(X) = \frac{1}{\lambda^2}}
\]

\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Zipf Distribution}

The Zipf distribution does not have a MGF, because the series for the MGF does not converge for any interval around zero. 
While it does not have a MGF, it does have a probability generating function (PGF) defined in terms of the polylogarithm function.

\begin{itemize}
    \item
    \textbf{Why the MGF does not exist:} 
        The definition of the MGF for the Zipf distribution involves an infinite series, and this series does not converge on an open interval that includes $0$. 
        For a MGF to exist, the series must converge in a neighborhood of $0$.
    \item
    \textbf{The PGF exists:}
        The Zipf distribution does have a probability generating function (PGF), which is defined as $E[z^x]$.
    \item
    \textbf{PGF Formula:}
        The PGF for the Zipf distribution is given by:
        \begin {center}
        	$ P(z) = \dfrac{ \operatorname{Li}_a(z)}{ \operatorname{Li}_a(1)} $ 
        	\quad where $\operatorname{Li}_a(z)$ is the polylogarithm function.
        	\end{center}
\end{itemize}

The polylogarithm function is given by:
\[
    \operatorname{Li}_s(z) = \sum\limits_{n=1}^\infty \dfrac{z^n}{n^s}
\]
\[
    \operatorname{Li}_s(z) = \dfrac{1}{ \Gamma(s)} \int_0^\infty \dfrac{t^{s - 1}}{\frac{e^t}{z} - 1} dt
\]

The Zipf distribution is used often, particularly for analyzing patterns in fields like linguistics, internet traffic, city populations, and economics, where a few high-frequency items dominate a much larger number of low-frequency ones. It's a fundamental concept for understanding how rank and frequency relate in many natural and human-made systems.

\pagebreak

\end{document}

