\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{multicol}
\usepackage{enumitem}
\usepackage{titlesec}
\usepackage{helvet}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{hyperref}
\titleformat{\section}{\large\bfseries}{}{0em}{}[\titlerule]
\titlespacing{\section}{0pt}{0.5em}{0.25em}
\setlength{\parskip}{0.2em}
\setlength{\parindent}{0em}
\setlist[itemize]{noitemsep, topsep=0pt, left=1em}
\usepackage[none]{hyphenat}
\pagenumbering{gobble}
\usepackage{amsmath}

\vspace*{-2cm}
\begin{document}

\section{Binomial: Mean and Variance from MGF}

\subsection{MGF of Binomial Distribution}
The moment generating function (MGF) of a binomial distribution is given by:
\[
M_X(t) = E[e^{tX}] = \sum_{x=0}^{n} e^{tx} P(X = x)
\]

We know that
\[
P(X = x) = \binom{n}{x} p^x q^{n-x}, \quad \text{where } q = 1 - p
\]
Substituting in:
\[
M_X(t) = \sum_{x=0}^{n} \binom{n}{x} (pe^t)^x q^{n-x}
\]
By the Binomial Theorem, this simplifies to:
\[
M_X(t) = (q + pe^t)^n
\]

\subsection{Raw Moments}
We can find the first and second raw moments by differentiating the MGF.

\textbf{First moment (mean):}
\[
\mu_1' = E[X] = \frac{dM_X(t)}{dt}\bigg|_{t=0}
\]
Differentiate:
\[
\frac{dM_X(t)}{dt} = n(q + pe^t)^{n-1}(pe^t)
\]
Now evaluate at $t = 0$:
\[
\mu_1' = np(q + p)^{n-1} = np(1)^{n-1} = np
\]
Therefore, the mean is:
\[
\boxed{E[X] = np}
\]

\textbf{Second moment:}
\[
\mu_2' = \frac{d^2 M_X(t)}{dt^2}\bigg|_{t=0}
\]
Differentiate again:
\[
\frac{d^2 M_X(t)}{dt^2} = np \left[ e^t (n-1)(q + pe^t)^{n-2}(pe^t) + e^t (q + pe^t)^{n-1} \right]
\]
Simplify and evaluate at $t = 0$:
\[
\mu_2' = np \left[ (n-1)p + 1 \right] = n(n-1)p^2 + np
\]

\subsection{Variance of Binomial Distribution}
The variance is given by:
\[
\sigma^2 = E[X^2] - (E[X])^2 = \mu_2' - (\mu_1')^2
\]
Substitute the results:
\[
\sigma^2 = \left[ n(n-1)p^2 + np \right] - (np)^2
\]
Simplify:
\[
\sigma^2 = n(n-1)p^2 + np - n^2p^2 = np(1 - p)
\]
Thus,
\[
\boxed{\sigma^2 = Var(X) = npq}
\]
\pagebreak 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bernoulli: Mean and Variance from MGF}
\subsection{MGF of Bernoulli Distribution}
Given the moment-generating function (MGF):
\[
    M_X(t) = 1 - p + pe^t 
\]

\subsection{Raw Moments}
Let's find the first moment:
\[
    \dfrac{d}{dt} M_X(t) = \dfrac{d}{dt} (1 - p + pe^t) = pe^t
\]
\[
 	M_X'(0) = pe^0 
\]
Therefore, the mean is:
\[
    \boxed{E[X] = p}
\]

Let's find the second moment:
\[
    \dfrac{d^2}{dt^2} M_X(t) = \dfrac{d}{dt} (pe^t) = pe^t
\]

\subsection{Variance of Bernoulli Distribution}
Let's find the variance: $\sigma^2 = E[X^2] - E[X]^2$. Substituting yields: \\
\[
    \sigma^2 = p - p^2 = p(1 - p) = pq
\]
Therefore, the variance is: \\
\[
	\boxed{\sigma^2 = pq}
\]

\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Poisson: Mean and Variance from MGF}
\subsection{MFG of Poisson Distribution}
\[
    M_X(t) = E(e^{tx}) = \int_{-\infty}^\infty e^(tx) f_X(x) dx
\]
\[
    M_X(t) = \int_0^\infty e^{tx} \lambda e ^ {-\lambda x } dx 
\]
\[
    M_X(t) = \lambda \int_0^\infty e^{tx - \lambda x} dx 
        = \lambda \int_0^\infty e^{x ( t - \lambda) } dx 
\]
\[
    M_X(t) = \lambda \dfrac{e^{x(t - \lambda)}}{(t - \lambda)} \bigg|_{0}^\infty
\]
\[
    M_X(t) = \lambda \dfrac{0 -1}{(t - \lambda)}
\]
\[
    M_X{t} = \dfrac{ \lambda } { \lambda - t }
\]

\subsection{Raw Moments}
TODO 

\subsection{Variance}
TODO 

\pagebreak

% \section{Geometric: Mean and Variance from MGF}
 %\section{HyperGeometric: Mean and Variance from MGF}
% \section{Rademacher: Mean and Variance from MGF}
% \section{Negative Binomial: Mean and Variance from MGF}
% \section{Negative HyperGeometric: Mean and Variance from MGF}

\end{document}

